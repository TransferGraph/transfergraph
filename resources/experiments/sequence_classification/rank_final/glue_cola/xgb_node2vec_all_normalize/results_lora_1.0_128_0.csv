,model,score
0,Jeevesh8/init_bert_ft_qqp-33,0.8104832
1,Jeevesh8/init_bert_ft_qqp-33,0.83619094
2,Jeevesh8/init_bert_ft_qqp-49,0.8270611
3,Jeevesh8/init_bert_ft_qqp-49,0.76299655
4,connectivity/bert_ft_qqp-7,0.72996944
5,connectivity/bert_ft_qqp-7,0.72336954
6,Jeevesh8/bert_ft_qqp-40,0.8473191
7,Jeevesh8/bert_ft_qqp-40,0.83326274
8,Jeevesh8/bert_ft_qqp-9,0.84994054
9,Jeevesh8/bert_ft_qqp-9,0.8039101
10,Jeevesh8/bert_ft_qqp-88,0.87465835
11,Jeevesh8/bert_ft_qqp-88,0.7422793
12,connectivity/bert_ft_qqp-25,0.8357425
13,connectivity/bert_ft_qqp-25,0.8257412
14,Jeevesh8/bert_ft_qqp-55,0.86271006
15,Jeevesh8/bert_ft_qqp-55,0.74609405
16,connectivity/bert_ft_qqp-1,0.86396694
17,connectivity/bert_ft_qqp-1,0.8145395
18,Jeevesh8/bert_ft_qqp-39,0.9039577
19,Jeevesh8/bert_ft_qqp-39,0.80767286
20,connectivity/bert_ft_qqp-94,0.8546367
21,connectivity/bert_ft_qqp-94,0.81783503
22,connectivity/bert_ft_qqp-96,0.8418394
23,connectivity/bert_ft_qqp-96,0.8187308
24,Jeevesh8/init_bert_ft_qqp-24,0.8261335
25,Jeevesh8/init_bert_ft_qqp-24,0.7339301
26,Jeevesh8/bert_ft_qqp-68,0.84889233
27,Jeevesh8/bert_ft_qqp-68,0.8040143
28,Jeevesh8/init_bert_ft_qqp-28,0.88533163
29,Jeevesh8/init_bert_ft_qqp-28,0.76960856
30,connectivity/bert_ft_qqp-17,0.81730723
31,connectivity/bert_ft_qqp-17,0.7424704
32,ali2066/finetuned_sentence_itr1_2e-05_all_26_02_2022-04_03_26,0.82013613
33,ali2066/finetuned_sentence_itr1_2e-05_all_26_02_2022-04_03_26,0.8162095
34,SetFit/distilbert-base-uncased__sst2__train-16-0,0.8228188
35,SetFit/distilbert-base-uncased__sst2__train-16-0,0.81898874
36,aviator-neural/bert-base-uncased-sst2,0.8761476
37,aviator-neural/bert-base-uncased-sst2,0.80941975
38,SetFit/distilbert-base-uncased__sst2__train-32-9,0.81802714
39,SetFit/distilbert-base-uncased__sst2__train-32-9,0.79994273
40,Alassea/glue_sst_classifier,0.836698
41,Alassea/glue_sst_classifier,0.8291938
42,philschmid/tiny-distilbert-classification,0.4704386
43,philschmid/tiny-distilbert-classification,0.4656659
44,moshew/bert-mini-sst2-distilled,0.8014135
45,moshew/bert-mini-sst2-distilled,0.7052969
46,ChrisUPM/BioBERT_Re_trained,0.8099579
47,ChrisUPM/BioBERT_Re_trained,0.77347755
48,vaariis/distilbert-base-uncased-finetuned-emotion,0.8375611
49,vaariis/distilbert-base-uncased-finetuned-emotion,0.8428276
50,marcelcastrobr/sagemaker-distilbert-emotion,0.8220483
51,marcelcastrobr/sagemaker-distilbert-emotion,0.8333022
52,jasonyim2/distilbert-base-uncased-finetuned-emotion,0.84747267
53,jasonyim2/distilbert-base-uncased-finetuned-emotion,0.826931
54,abdelkader/distilbert-base-uncased-finetuned-emotion,0.829554
55,abdelkader/distilbert-base-uncased-finetuned-emotion,0.8019813
56,moghis/distilbert-base-uncased-finetuned-emotion,0.8330731
57,moghis/distilbert-base-uncased-finetuned-emotion,0.8027755
58,uygarkurt/distilbert-base-uncased-finetuned-emotion,0.8261815
59,uygarkurt/distilbert-base-uncased-finetuned-emotion,0.79513377
60,neibla/distilbert-base-uncased-finetuned-emotion,0.84396064
61,neibla/distilbert-base-uncased-finetuned-emotion,0.7750865
62,JB173/distilbert-base-uncased-finetuned-emotion,0.8388194
63,JB173/distilbert-base-uncased-finetuned-emotion,0.82529145
64,Nanatan/distilbert-base-uncased-finetuned-emotion,0.8145502
65,Nanatan/distilbert-base-uncased-finetuned-emotion,0.80794704
66,heranm/finetuning-sentiment-model-3000-samples,0.8309653
67,heranm/finetuning-sentiment-model-3000-samples,0.8184327
68,PrasunMishra/finetuning-sentiment-model-3000-samples,0.8428818
69,PrasunMishra/finetuning-sentiment-model-3000-samples,0.8445755
70,yukta10/finetuning-sentiment-model-3000-samples,0.83923787
71,yukta10/finetuning-sentiment-model-3000-samples,0.823191
72,ncduy/roberta-imdb-sentiment-analysis,0.8648145
73,ncduy/roberta-imdb-sentiment-analysis,0.8315974
74,markt23917/finetuning-sentiment-model-3000-samples,0.8263602
75,markt23917/finetuning-sentiment-model-3000-samples,0.80450433
76,juliensimon/autonlp-imdb-demo-hf-16622767,0.8395699
77,juliensimon/autonlp-imdb-demo-hf-16622767,0.8355222
78,fabriceyhc/bert-base-uncased-imdb,0.8187381
79,fabriceyhc/bert-base-uncased-imdb,0.7531829
80,riyadhctg/distilbert-base-uncased-finetuned-cola,0.8305882
81,riyadhctg/distilbert-base-uncased-finetuned-cola,0.82075834
82,connectivity/cola_6ep_ft-33,0.82292163
83,connectivity/cola_6ep_ft-33,0.80365056
84,connectivity/cola_6ep_ft-22,0.8490338
85,connectivity/cola_6ep_ft-22,0.8290009
86,isakbos/Q8BERT_COLA_L_512,0.63670695
87,isakbos/Q8BERT_COLA_L_512,0.6162896
88,jaesun/distilbert-base-uncased-finetuned-cola,0.8218751
89,jaesun/distilbert-base-uncased-finetuned-cola,0.79465055
90,usami/distilbert-base-uncased-finetuned-cola,0.8230149
91,usami/distilbert-base-uncased-finetuned-cola,0.79827756
92,Jeevesh8/512seq_len_6ep_bert_ft_cola-91,0.87323904
93,Jeevesh8/512seq_len_6ep_bert_ft_cola-91,0.77397186
94,Jeevesh8/6ep_bert_ft_cola-47,0.88808197
95,Jeevesh8/6ep_bert_ft_cola-47,0.82410896
96,connectivity/cola_6ep_ft-10,0.83909595
97,connectivity/cola_6ep_ft-10,0.83028835
98,Jeevesh8/6ep_bert_ft_cola-12,0.8527836
99,Jeevesh8/6ep_bert_ft_cola-12,0.8269375
100,Jeevesh8/bert_ft_cola-88,0.8526531
101,Jeevesh8/bert_ft_cola-88,0.8243974
102,Jeevesh8/6ep_bert_ft_cola-29,0.84239316
103,Jeevesh8/6ep_bert_ft_cola-29,0.8264995
104,vesteinn/XLMR-ENIS-finetuned-cola,0.8337155
105,vesteinn/XLMR-ENIS-finetuned-cola,0.79433906
106,navsad/navid_test_bert,0.8435672
107,navsad/navid_test_bert,0.7543211
108,Jeevesh8/bert_ft_cola-60,0.913376
109,Jeevesh8/bert_ft_cola-60,0.8148151
110,dapang/distilroberta-base-mic-sym,0.82085145
111,dapang/distilroberta-base-mic-sym,0.81101424
112,Capreolus/bert-base-msmarco,0.83438957
113,Capreolus/bert-base-msmarco,0.80839086
114,Splend1dchan/bert-base-uncased-slue-goldtrascription-e3-lr1e-4,0.84422874
115,Splend1dchan/bert-base-uncased-slue-goldtrascription-e3-lr1e-4,0.83304936
116,Jeevesh8/feather_berts_46,0.8774397
117,Jeevesh8/feather_berts_46,0.81443167
118,JonatanGk/roberta-base-bne-finetuned-cyberbullying-spanish,0.68333024
119,JonatanGk/roberta-base-bne-finetuned-cyberbullying-spanish,0.61378336
120,cambridgeltl/guardian_news_distilbert-base-uncased,0.808046
121,cambridgeltl/guardian_news_distilbert-base-uncased,0.75666136
122,amyma21/sincere_question_classification,0.81767476
123,amyma21/sincere_question_classification,0.8133146
124,phailyoor/distilbert-base-uncased-finetuned-yahd,0.7872992
125,phailyoor/distilbert-base-uncased-finetuned-yahd,0.7648626
126,arianpasquali/distilbert-base-multilingual-cased-toxicity,0.77211463
127,arianpasquali/distilbert-base-multilingual-cased-toxicity,0.7484041
128,connectivity/feather_berts_28,0.8506573
129,connectivity/feather_berts_28,0.8161336
130,liangyuant/distilbert-base-uncased-finetuned-num200-450-405cls,0.81970394
131,liangyuant/distilbert-base-uncased-finetuned-num200-450-405cls,0.83152264
132,DoyyingFace/bert-asian-hate-tweets-asian-unclean-freeze-4,0.85713303
133,DoyyingFace/bert-asian-hate-tweets-asian-unclean-freeze-4,0.8381797
134,Jeevesh8/lecun_feather_berts-3,0.8477173
135,Jeevesh8/lecun_feather_berts-3,0.79613024
136,Giyaseddin/distilroberta-base-finetuned-short-answer-assessment,0.80887425
137,Giyaseddin/distilroberta-base-finetuned-short-answer-assessment,0.80339754
138,AnonymousSub/dummy_2,0.7890532
139,AnonymousSub/dummy_2,0.73129565
140,Jeevesh8/lecun_feather_berts-51,0.853542
141,Jeevesh8/lecun_feather_berts-51,0.83163774
142,viviastaari/finetuning-sentiment-analysis-en-id,0.75474083
143,viviastaari/finetuning-sentiment-analysis-en-id,0.6594499
144,Aureliano/distilbert-base-uncased-if,0.80767554
145,Aureliano/distilbert-base-uncased-if,0.8033527
146,rmihaylov/roberta-base-sentiment-bg,0.7618733
147,rmihaylov/roberta-base-sentiment-bg,0.6562543
148,cardiffnlp/twitter-roberta-base-2021-124m,0.847621
149,cardiffnlp/twitter-roberta-base-2021-124m,0.83801526
150,Jeevesh8/lecun_feather_berts-8,0.83899
151,Jeevesh8/lecun_feather_berts-8,0.78503627
152,korca/bae-roberta-base-boolq,0.8265096
153,korca/bae-roberta-base-boolq,0.79656684
154,aditeyabaral/finetuned-sail2017-xlm-roberta-base,0.8815283
155,aditeyabaral/finetuned-sail2017-xlm-roberta-base,0.8228361
156,joebobby/finetuning-sentiment-model-5000-samples3,0.8196695
157,joebobby/finetuning-sentiment-model-5000-samples3,0.80501723
158,Jeevesh8/feather_berts_92,0.85751903
159,Jeevesh8/feather_berts_92,0.8486683
160,ASCCCCCCCC/distilbert-base-chinese-amazon_zh_20000,0.77374375
161,ASCCCCCCCC/distilbert-base-chinese-amazon_zh_20000,0.7544674
162,matthewburke/korean_sentiment,0.7208422
163,matthewburke/korean_sentiment,0.6404969
164,IMSyPP/hate_speech_nl,0.6483826
165,IMSyPP/hate_speech_nl,0.6065776
166,cointegrated/roberta-base-formality,0.84334147
167,cointegrated/roberta-base-formality,0.83642805
168,IMSyPP/hate_speech_it,0.74207807
169,IMSyPP/hate_speech_it,0.7221833
170,18811449050/bert_finetuning_test,0.8816592
171,18811449050/bert_finetuning_test,0.8260955
172,finiteautomata/betonews-tweetcontext,0.71063906
173,finiteautomata/betonews-tweetcontext,0.63119304
174,Jeevesh8/feather_berts_96,0.86732733
175,Jeevesh8/feather_berts_96,0.8319542
176,Jeevesh8/lecun_feather_berts-7,0.8414199
177,Jeevesh8/lecun_feather_berts-7,0.79987425
178,mrm8488/codebert-base-finetuned-detect-insecure-code,0.7869167
179,mrm8488/codebert-base-finetuned-detect-insecure-code,0.7372063
180,socialmediaie/TRAC2020_IBEN_B_bert-base-multilingual-uncased,0.8254893
181,socialmediaie/TRAC2020_IBEN_B_bert-base-multilingual-uncased,0.69533736
182,zenkri/autotrain-Arabic_Poetry_by_Subject-920730230,0.8364304
183,zenkri/autotrain-Arabic_Poetry_by_Subject-920730230,0.7664751
184,M47Labs/spanish_news_classification_headlines_untrained,0.7511787
185,M47Labs/spanish_news_classification_headlines_untrained,0.6099131
186,bondi/bert-semaphore-prediction-w4,0.733541
187,bondi/bert-semaphore-prediction-w4,0.695813
188,classla/bcms-bertic-parlasent-bcs-ter,0.70363784
189,classla/bcms-bertic-parlasent-bcs-ter,0.76784515
190,anferico/bert-for-patents,0.85319537
191,anferico/bert-for-patents,0.7467722
192,ali2066/DistilBERTFINAL_ctxSentence_TRAIN_editorials_TEST_NULL_second_train_set_null_False,0.8757777
193,ali2066/DistilBERTFINAL_ctxSentence_TRAIN_editorials_TEST_NULL_second_train_set_null_False,0.8435321
194,anvay/finetuning-cardiffnlp-sentiment-model,0.8851289
195,anvay/finetuning-cardiffnlp-sentiment-model,0.83857775
196,JonatanGk/roberta-base-ca-finetuned-hate-speech-offensive-catalan,0.6888711
197,JonatanGk/roberta-base-ca-finetuned-hate-speech-offensive-catalan,0.6160241
198,Raychanan/COVID_RandomOver,0.5776942
199,Raychanan/COVID_RandomOver,0.54084516
200,kyleinincubated/autonlp-cat333-624217911,0.80047673
201,kyleinincubated/autonlp-cat333-624217911,0.74146056
202,cardiffnlp/bertweet-base-stance-climate,0.84947264
203,cardiffnlp/bertweet-base-stance-climate,0.79635286
204,nurkayevaa/autonlp-bert-covid-407910458,0.80551744
205,nurkayevaa/autonlp-bert-covid-407910458,0.81377923
206,SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-1,0.86247206
207,SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-1,0.81367445
208,crcb/isear_bert,0.8407904
209,crcb/isear_bert,0.83004546
210,robertou2/roberta-base-bne-finetuned-amazon_reviews_multi,0.76478696
211,robertou2/roberta-base-bne-finetuned-amazon_reviews_multi,0.7359962
212,milyiyo/selectra-small-finetuned-amazon-review,0.6956862
213,milyiyo/selectra-small-finetuned-amazon-review,0.66850454
214,Anthos23/FS-distilroberta-fine-tuned,0.8387808
215,Anthos23/FS-distilroberta-fine-tuned,0.8288414
216,oferweintraub/bert-base-finance-sentiment-noisy-search,0.9014251
217,oferweintraub/bert-base-finance-sentiment-noisy-search,0.82455486
218,pietrotrope/emotion_final,0.8635564
219,pietrotrope/emotion_final,0.8161242
220,aXhyra/emotion_trained_31415,0.824163
221,aXhyra/emotion_trained_31415,0.843058
222,aXhyra/presentation_emotion_31415,0.8312867
223,aXhyra/presentation_emotion_31415,0.84198755
224,Recognai/bert-base-spanish-wwm-cased-xnli,0.7407652
225,Recognai/bert-base-spanish-wwm-cased-xnli,0.70798075
226,anirudh21/bert-base-uncased-finetuned-qnli,0.84873664
227,anirudh21/bert-base-uncased-finetuned-qnli,0.8173066
228,aXhyra/demo_sentiment_31415,0.8031059
229,aXhyra/demo_sentiment_31415,0.8088592
230,aXhyra/presentation_sentiment_1234567,0.8066529
231,aXhyra/presentation_sentiment_1234567,0.7981342
232,jb2k/bert-base-multilingual-cased-language-detection,0.801857
233,jb2k/bert-base-multilingual-cased-language-detection,0.6475571
234,vinai/bertweet-base,0.86271137
235,vinai/bertweet-base,0.81691897
236,justinqbui/bertweet-covid19-base-uncased-pretraining-covid-vaccine-tweets,0.83039576
237,justinqbui/bertweet-covid19-base-uncased-pretraining-covid-vaccine-tweets,0.81886756
238,vinai/bertweet-covid19-base-cased,0.8342546
239,vinai/bertweet-covid19-base-cased,0.8126756
240,vinai/bertweet-covid19-base-uncased,0.8497549
241,vinai/bertweet-covid19-base-uncased,0.838858
242,distilbert-base-uncased,0.86103755
243,distilbert-base-uncased,0.82261807
244,bert-base-uncased,0.841279
245,bert-base-uncased,0.7919574
246,roberta-base,0.87500733
247,roberta-base,0.81691563
248,bert-base-cased,0.8382749
249,bert-base-cased,0.8164585
250,dhimskyy/wiki-bert,0.6961488
251,dhimskyy/wiki-bert,0.56964505
252,michiyasunaga/LinkBERT-base,0.82498544
253,michiyasunaga/LinkBERT-base,0.78434443
254,bert-large-uncased,0.82891
255,bert-large-uncased,0.77153337
256,roberta-large,0.78899056
257,roberta-large,0.8946951
258,boychaboy/MNLI_roberta-base,0.859483
259,boychaboy/MNLI_roberta-base,0.84969544
260,ishan/bert-base-uncased-mnli,0.8630192
261,ishan/bert-base-uncased-mnli,0.8510422
262,emrecan/bert-base-multilingual-cased-snli_tr,0.8403886
263,emrecan/bert-base-multilingual-cased-snli_tr,0.77645713
264,elozano/tweet_offensive_eval,0.68772215
265,elozano/tweet_offensive_eval,0.64675814
266,Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two,0.9068348
267,Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two,0.7646379
268,aychang/bert-base-cased-trec-coarse,0.8504873
269,aychang/bert-base-cased-trec-coarse,0.8378226
270,gchhablani/bert-base-cased-finetuned-wnli,0.83270705
271,gchhablani/bert-base-cased-finetuned-wnli,0.78801954
272,w11wo/sundanese-bert-base-emotion-classifier,0.6702761
273,w11wo/sundanese-bert-base-emotion-classifier,0.60269016
274,gchhablani/bert-base-cased-finetuned-rte,0.843799
275,gchhablani/bert-base-cased-finetuned-rte,0.85685134
276,mrm8488/electricidad-base-finetuned-pawsx-es,0.6963163
277,mrm8488/electricidad-base-finetuned-pawsx-es,0.63383734
278,manueltonneau/bert-twitter-en-is-hired,0.8427797
279,manueltonneau/bert-twitter-en-is-hired,0.7992365
280,Guscode/DKbert-hatespeech-detection,0.7066712
281,Guscode/DKbert-hatespeech-detection,0.6117371
