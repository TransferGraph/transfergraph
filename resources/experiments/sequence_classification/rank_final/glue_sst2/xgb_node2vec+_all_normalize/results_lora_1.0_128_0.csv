,model,score
0,Jeevesh8/init_bert_ft_qqp-33,0.9274012
1,Jeevesh8/init_bert_ft_qqp-33,0.9171673
2,Jeevesh8/init_bert_ft_qqp-33,0.8825519
3,Jeevesh8/init_bert_ft_qqp-49,0.9127702
4,Jeevesh8/init_bert_ft_qqp-49,0.8607812
5,connectivity/bert_ft_qqp-7,0.89199746
6,connectivity/bert_ft_qqp-7,0.85540444
7,Jeevesh8/bert_ft_qqp-40,0.9275288
8,Jeevesh8/bert_ft_qqp-40,0.86003494
9,Jeevesh8/bert_ft_qqp-9,0.92667764
10,Jeevesh8/bert_ft_qqp-9,0.8995123
11,Jeevesh8/bert_ft_qqp-88,0.92806786
12,Jeevesh8/bert_ft_qqp-88,0.88769335
13,connectivity/bert_ft_qqp-25,0.9388027
14,connectivity/bert_ft_qqp-25,0.8974027
15,Jeevesh8/bert_ft_qqp-55,0.9292218
16,Jeevesh8/bert_ft_qqp-55,0.88832134
17,connectivity/bert_ft_qqp-1,0.9135803
18,connectivity/bert_ft_qqp-1,0.8681339
19,Jeevesh8/bert_ft_qqp-39,0.92935044
20,Jeevesh8/bert_ft_qqp-39,0.892084
21,connectivity/bert_ft_qqp-94,0.94069296
22,connectivity/bert_ft_qqp-94,0.90387017
23,connectivity/bert_ft_qqp-96,0.92266035
24,connectivity/bert_ft_qqp-96,0.8843239
25,Jeevesh8/init_bert_ft_qqp-24,0.92777354
26,Jeevesh8/init_bert_ft_qqp-24,0.87615985
27,Jeevesh8/bert_ft_qqp-68,0.9298542
28,Jeevesh8/bert_ft_qqp-68,0.88481665
29,Jeevesh8/init_bert_ft_qqp-28,0.91202134
30,Jeevesh8/init_bert_ft_qqp-28,0.88591117
31,Jeevesh8/init_bert_ft_qqp-28,0.88591117
32,connectivity/bert_ft_qqp-17,0.9234276
33,connectivity/bert_ft_qqp-17,0.88296396
34,ali2066/finetuned_sentence_itr1_2e-05_all_26_02_2022-04_03_26,0.9098979
35,ali2066/finetuned_sentence_itr1_2e-05_all_26_02_2022-04_03_26,0.9162484
36,SetFit/distilbert-base-uncased__sst2__train-16-0,0.8782778
37,SetFit/distilbert-base-uncased__sst2__train-16-0,0.88873816
38,aviator-neural/bert-base-uncased-sst2,0.93087894
39,aviator-neural/bert-base-uncased-sst2,0.897483
40,SetFit/distilbert-base-uncased__sst2__train-32-9,0.89306223
41,SetFit/distilbert-base-uncased__sst2__train-32-9,0.9011371
42,Alassea/glue_sst_classifier,0.9281327
43,Alassea/glue_sst_classifier,0.8931551
44,philschmid/tiny-distilbert-classification,0.38341826
45,philschmid/tiny-distilbert-classification,0.35276657
46,moshew/bert-mini-sst2-distilled,0.8870942
47,moshew/bert-mini-sst2-distilled,0.8598717
48,ChrisUPM/BioBERT_Re_trained,0.89356655
49,ChrisUPM/BioBERT_Re_trained,0.858333
50,vaariis/distilbert-base-uncased-finetuned-emotion,0.86891276
51,vaariis/distilbert-base-uncased-finetuned-emotion,0.874902
52,marcelcastrobr/sagemaker-distilbert-emotion,0.86976534
53,marcelcastrobr/sagemaker-distilbert-emotion,0.87709665
54,jasonyim2/distilbert-base-uncased-finetuned-emotion,0.8746269
55,jasonyim2/distilbert-base-uncased-finetuned-emotion,0.88364357
56,abdelkader/distilbert-base-uncased-finetuned-emotion,0.88845414
57,abdelkader/distilbert-base-uncased-finetuned-emotion,0.8957866
58,moghis/distilbert-base-uncased-finetuned-emotion,0.87029564
59,moghis/distilbert-base-uncased-finetuned-emotion,0.8773075
60,uygarkurt/distilbert-base-uncased-finetuned-emotion,0.8839337
61,uygarkurt/distilbert-base-uncased-finetuned-emotion,0.88735217
62,neibla/distilbert-base-uncased-finetuned-emotion,0.87056243
63,neibla/distilbert-base-uncased-finetuned-emotion,0.8687599
64,JB173/distilbert-base-uncased-finetuned-emotion,0.88176787
65,JB173/distilbert-base-uncased-finetuned-emotion,0.87912995
66,Nanatan/distilbert-base-uncased-finetuned-emotion,0.86814576
67,Nanatan/distilbert-base-uncased-finetuned-emotion,0.8754652
68,heranm/finetuning-sentiment-model-3000-samples,0.87055016
69,heranm/finetuning-sentiment-model-3000-samples,0.87781566
70,PrasunMishra/finetuning-sentiment-model-3000-samples,0.8979936
71,PrasunMishra/finetuning-sentiment-model-3000-samples,0.9049962
72,yukta10/finetuning-sentiment-model-3000-samples,0.889947
73,yukta10/finetuning-sentiment-model-3000-samples,0.8895466
74,ncduy/roberta-imdb-sentiment-analysis,0.9544716
75,ncduy/roberta-imdb-sentiment-analysis,0.9102758
76,markt23917/finetuning-sentiment-model-3000-samples,0.8729366
77,markt23917/finetuning-sentiment-model-3000-samples,0.8805348
78,juliensimon/autonlp-imdb-demo-hf-16622767,0.8946054
79,juliensimon/autonlp-imdb-demo-hf-16622767,0.9039816
80,fabriceyhc/bert-base-uncased-imdb,0.9274756
81,fabriceyhc/bert-base-uncased-imdb,0.87504643
82,riyadhctg/distilbert-base-uncased-finetuned-cola,0.8686384
83,riyadhctg/distilbert-base-uncased-finetuned-cola,0.8737199
84,connectivity/cola_6ep_ft-33,0.93131816
85,connectivity/cola_6ep_ft-33,0.8979301
86,connectivity/cola_6ep_ft-22,0.9268292
87,connectivity/cola_6ep_ft-22,0.8821944
88,isakbos/Q8BERT_COLA_L_512,0.8084842
89,isakbos/Q8BERT_COLA_L_512,0.7377925
90,jaesun/distilbert-base-uncased-finetuned-cola,0.85226667
91,jaesun/distilbert-base-uncased-finetuned-cola,0.85382944
92,usami/distilbert-base-uncased-finetuned-cola,0.83444816
93,usami/distilbert-base-uncased-finetuned-cola,0.8634093
94,Jeevesh8/512seq_len_6ep_bert_ft_cola-91,0.9184065
95,Jeevesh8/512seq_len_6ep_bert_ft_cola-91,0.87488604
96,Jeevesh8/6ep_bert_ft_cola-47,0.9220541
97,Jeevesh8/6ep_bert_ft_cola-47,0.88698024
98,connectivity/cola_6ep_ft-10,0.9262752
99,connectivity/cola_6ep_ft-10,0.8853285
100,Jeevesh8/6ep_bert_ft_cola-12,0.90890497
101,Jeevesh8/6ep_bert_ft_cola-12,0.870497
102,Jeevesh8/bert_ft_cola-88,0.91816646
103,Jeevesh8/bert_ft_cola-88,0.8695801
104,Jeevesh8/6ep_bert_ft_cola-29,0.91719806
105,Jeevesh8/6ep_bert_ft_cola-29,0.8846428
106,vesteinn/XLMR-ENIS-finetuned-cola,0.9177951
107,vesteinn/XLMR-ENIS-finetuned-cola,0.8627465
108,navsad/navid_test_bert,0.92228556
109,navsad/navid_test_bert,0.8783359
110,Jeevesh8/bert_ft_cola-60,0.9245772
111,Jeevesh8/bert_ft_cola-60,0.87654847
112,dapang/distilroberta-base-mic-sym,0.8912688
113,dapang/distilroberta-base-mic-sym,0.89489824
114,Capreolus/bert-base-msmarco,0.91809267
115,Capreolus/bert-base-msmarco,0.89380133
116,Splend1dchan/bert-base-uncased-slue-goldtrascription-e3-lr1e-4,0.9141624
117,Splend1dchan/bert-base-uncased-slue-goldtrascription-e3-lr1e-4,0.87144125
118,Jeevesh8/feather_berts_46,0.9230103
119,Jeevesh8/feather_berts_46,0.88320947
120,JonatanGk/roberta-base-bne-finetuned-cyberbullying-spanish,0.8306715
121,JonatanGk/roberta-base-bne-finetuned-cyberbullying-spanish,0.7476242
122,cambridgeltl/guardian_news_distilbert-base-uncased,0.8558893
123,cambridgeltl/guardian_news_distilbert-base-uncased,0.8586491
124,amyma21/sincere_question_classification,0.8899797
125,amyma21/sincere_question_classification,0.8949875
126,phailyoor/distilbert-base-uncased-finetuned-yahd,0.8713381
127,phailyoor/distilbert-base-uncased-finetuned-yahd,0.8824061
128,arianpasquali/distilbert-base-multilingual-cased-toxicity,0.84643704
129,arianpasquali/distilbert-base-multilingual-cased-toxicity,0.86738634
130,connectivity/feather_berts_28,0.93741643
131,connectivity/feather_berts_28,0.90343505
132,liangyuant/distilbert-base-uncased-finetuned-num200-450-405cls,0.88989335
133,liangyuant/distilbert-base-uncased-finetuned-num200-450-405cls,0.88925004
134,DoyyingFace/bert-asian-hate-tweets-asian-unclean-freeze-4,0.94765323
135,DoyyingFace/bert-asian-hate-tweets-asian-unclean-freeze-4,0.9047899
136,Jeevesh8/lecun_feather_berts-3,0.9211479
137,Jeevesh8/lecun_feather_berts-3,0.8778371
138,Giyaseddin/distilroberta-base-finetuned-short-answer-assessment,0.8951438
139,Giyaseddin/distilroberta-base-finetuned-short-answer-assessment,0.8955764
140,AnonymousSub/dummy_2,0.9049547
141,AnonymousSub/dummy_2,0.85103303
142,Jeevesh8/lecun_feather_berts-51,0.91882205
143,Jeevesh8/lecun_feather_berts-51,0.8905834
144,viviastaari/finetuning-sentiment-analysis-en-id,0.84569937
145,viviastaari/finetuning-sentiment-analysis-en-id,0.8630138
146,Aureliano/distilbert-base-uncased-if,0.8607142
147,Aureliano/distilbert-base-uncased-if,0.87241393
148,rmihaylov/roberta-base-sentiment-bg,0.8635019
149,rmihaylov/roberta-base-sentiment-bg,0.86677325
150,cardiffnlp/twitter-roberta-base-2021-124m,0.9380741
151,cardiffnlp/twitter-roberta-base-2021-124m,0.88625926
152,Jeevesh8/lecun_feather_berts-8,0.9261917
153,Jeevesh8/lecun_feather_berts-8,0.896543
154,korca/bae-roberta-base-boolq,0.93294936
155,korca/bae-roberta-base-boolq,0.87361294
156,aditeyabaral/finetuned-sail2017-xlm-roberta-base,0.92976177
157,aditeyabaral/finetuned-sail2017-xlm-roberta-base,0.8679041
158,joebobby/finetuning-sentiment-model-5000-samples3,0.94244224
159,joebobby/finetuning-sentiment-model-5000-samples3,0.890791
160,Jeevesh8/feather_berts_92,0.918464
161,Jeevesh8/feather_berts_92,0.8887806
162,ASCCCCCCCC/distilbert-base-chinese-amazon_zh_20000,0.86083454
163,ASCCCCCCCC/distilbert-base-chinese-amazon_zh_20000,0.79416454
164,matthewburke/korean_sentiment,0.86308306
165,matthewburke/korean_sentiment,0.8045017
166,IMSyPP/hate_speech_nl,0.83434063
167,IMSyPP/hate_speech_nl,0.7766621
168,cointegrated/roberta-base-formality,0.92387605
169,cointegrated/roberta-base-formality,0.88179255
170,IMSyPP/hate_speech_it,0.9198692
171,IMSyPP/hate_speech_it,0.8528075
172,18811449050/bert_finetuning_test,0.9011396
173,18811449050/bert_finetuning_test,0.87635314
174,finiteautomata/betonews-tweetcontext,0.88359374
175,finiteautomata/betonews-tweetcontext,0.8012046
176,Jeevesh8/feather_berts_96,0.9336885
177,Jeevesh8/feather_berts_96,0.9093374
178,Jeevesh8/lecun_feather_berts-7,0.93288964
179,Jeevesh8/lecun_feather_berts-7,0.895565
180,mrm8488/codebert-base-finetuned-detect-insecure-code,0.9258934
181,mrm8488/codebert-base-finetuned-detect-insecure-code,0.8798494
182,socialmediaie/TRAC2020_IBEN_B_bert-base-multilingual-uncased,0.9329545
183,socialmediaie/TRAC2020_IBEN_B_bert-base-multilingual-uncased,0.8727141
184,zenkri/autotrain-Arabic_Poetry_by_Subject-920730230,0.93977904
185,zenkri/autotrain-Arabic_Poetry_by_Subject-920730230,0.8871029
186,M47Labs/spanish_news_classification_headlines_untrained,0.90678537
187,M47Labs/spanish_news_classification_headlines_untrained,0.8105372
188,bondi/bert-semaphore-prediction-w4,0.8752526
189,bondi/bert-semaphore-prediction-w4,0.81725436
190,classla/bcms-bertic-parlasent-bcs-ter,0.8942885
191,classla/bcms-bertic-parlasent-bcs-ter,0.83299536
192,anferico/bert-for-patents,0.93272275
193,anferico/bert-for-patents,0.9274991
194,ali2066/DistilBERTFINAL_ctxSentence_TRAIN_editorials_TEST_NULL_second_train_set_null_False,0.947628
195,ali2066/DistilBERTFINAL_ctxSentence_TRAIN_editorials_TEST_NULL_second_train_set_null_False,0.9139182
196,anvay/finetuning-cardiffnlp-sentiment-model,0.9427111
197,anvay/finetuning-cardiffnlp-sentiment-model,0.90291655
198,JonatanGk/roberta-base-ca-finetuned-hate-speech-offensive-catalan,0.87678653
199,JonatanGk/roberta-base-ca-finetuned-hate-speech-offensive-catalan,0.7998973
200,Raychanan/COVID_RandomOver,0.4710539
201,Raychanan/COVID_RandomOver,0.47482324
202,kyleinincubated/autonlp-cat333-624217911,0.87337554
203,kyleinincubated/autonlp-cat333-624217911,0.7896886
204,cardiffnlp/bertweet-base-stance-climate,0.94649744
205,cardiffnlp/bertweet-base-stance-climate,0.9039891
206,nurkayevaa/autonlp-bert-covid-407910458,0.8609493
207,nurkayevaa/autonlp-bert-covid-407910458,0.8885279
208,SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-1,0.8705421
209,SetFit/distilbert-base-uncased__hate_speech_offensive__train-16-1,0.87772965
210,crcb/isear_bert,0.92165226
211,crcb/isear_bert,0.88688815
212,robertou2/roberta-base-bne-finetuned-amazon_reviews_multi,0.873745
213,robertou2/roberta-base-bne-finetuned-amazon_reviews_multi,0.812677
214,milyiyo/selectra-small-finetuned-amazon-review,0.8064372
215,milyiyo/selectra-small-finetuned-amazon-review,0.8130949
216,Anthos23/FS-distilroberta-fine-tuned,0.88715917
217,Anthos23/FS-distilroberta-fine-tuned,0.9004435
218,oferweintraub/bert-base-finance-sentiment-noisy-search,0.9308513
219,oferweintraub/bert-base-finance-sentiment-noisy-search,0.89219445
220,pietrotrope/emotion_final,0.89643383
221,pietrotrope/emotion_final,0.90248704
222,aXhyra/emotion_trained_31415,0.9293962
223,aXhyra/emotion_trained_31415,0.8900922
224,aXhyra/presentation_emotion_31415,0.8834045
225,aXhyra/presentation_emotion_31415,0.88603675
226,Recognai/bert-base-spanish-wwm-cased-xnli,0.8714767
227,Recognai/bert-base-spanish-wwm-cased-xnli,0.7744553
228,anirudh21/bert-base-uncased-finetuned-qnli,0.94265395
229,anirudh21/bert-base-uncased-finetuned-qnli,0.9053376
230,aXhyra/demo_sentiment_31415,0.8819949
231,aXhyra/demo_sentiment_31415,0.8924443
232,aXhyra/presentation_sentiment_1234567,0.88712084
233,aXhyra/presentation_sentiment_1234567,0.90077764
234,jb2k/bert-base-multilingual-cased-language-detection,0.9422204
235,jb2k/bert-base-multilingual-cased-language-detection,0.86410797
236,vinai/bertweet-base,0.93603665
237,vinai/bertweet-base,0.8932043
238,justinqbui/bertweet-covid19-base-uncased-pretraining-covid-vaccine-tweets,0.9272127
239,justinqbui/bertweet-covid19-base-uncased-pretraining-covid-vaccine-tweets,0.8863477
240,vinai/bertweet-covid19-base-cased,0.81720936
241,vinai/bertweet-covid19-base-cased,0.93210274
242,vinai/bertweet-covid19-base-uncased,0.9394818
243,vinai/bertweet-covid19-base-uncased,0.90018624
244,distilbert-base-uncased,0.8933859
245,distilbert-base-uncased,0.89633626
246,bert-base-uncased,0.9458925
247,bert-base-uncased,0.91433597
248,roberta-base,0.93207073
249,roberta-base,0.88394547
250,bert-base-cased,0.9170279
251,bert-base-cased,0.8841595
252,dhimskyy/wiki-bert,0.8735122
253,dhimskyy/wiki-bert,0.8351202
254,michiyasunaga/LinkBERT-base,0.90607
255,michiyasunaga/LinkBERT-base,0.8775836
256,bert-large-uncased,0.92438465
257,bert-large-uncased,0.9165791
258,roberta-large,0.919228
259,roberta-large,0.9000175
260,boychaboy/MNLI_roberta-base,0.941364
261,boychaboy/MNLI_roberta-base,0.9091926
262,ishan/bert-base-uncased-mnli,0.911623
263,ishan/bert-base-uncased-mnli,0.8796405
264,emrecan/bert-base-multilingual-cased-snli_tr,0.91967005
265,emrecan/bert-base-multilingual-cased-snli_tr,0.86968136
266,elozano/tweet_offensive_eval,0.85992235
267,elozano/tweet_offensive_eval,0.8029297
268,Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two,0.92330056
269,Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two,0.89060926
270,aychang/bert-base-cased-trec-coarse,0.9346629
271,aychang/bert-base-cased-trec-coarse,0.89471173
272,gchhablani/bert-base-cased-finetuned-wnli,0.916457
273,gchhablani/bert-base-cased-finetuned-wnli,0.8749505
274,w11wo/sundanese-bert-base-emotion-classifier,0.8599989
275,w11wo/sundanese-bert-base-emotion-classifier,0.7909749
276,gchhablani/bert-base-cased-finetuned-rte,0.92426085
277,gchhablani/bert-base-cased-finetuned-rte,0.8861498
278,mrm8488/electricidad-base-finetuned-pawsx-es,0.8686721
279,mrm8488/electricidad-base-finetuned-pawsx-es,0.8082915
280,manueltonneau/bert-twitter-en-is-hired,0.9218272
281,manueltonneau/bert-twitter-en-is-hired,0.8910395
282,Guscode/DKbert-hatespeech-detection,0.8384525
283,Guscode/DKbert-hatespeech-detection,0.78873694
